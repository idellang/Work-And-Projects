---
title: "Getting and Cleaning data for CICO matching"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load libraries and establish connection
```{r}
library(elastic)
library(elasticsearchr)
library(jsonlite)
library(tidyverse)
library(tidyr)
library(broom)
library(data.table)
library(lubridate)
library(getPass)

conn = connect(
host = "tmsuite-elasticsearch.trackme.com.ph",
port = 443,
path = NULL,
transport_schema = "https",
user = 'jfcastaneda',
pwd =  getPass("Enter Password:"),
headers = NULL,
cainfo = NULL,
force = FALSE,
errors = "simple",
warn = TRUE
)
```

Query
```{r}
bulk_bookings = "{\"size\": 0, \"query\": {\"bool\": {\"must\": [], \"filter\": [{\"match_all\": {}}, {\"match_phrase\": {\"group_names\": \"Nestle\"}}, {\"match_phrase\": {\"client_name\": \"Pan Logistics\"}}, {\"match_phrase\": {\"data_complete\": \"true\"}}, {\"nested\": {\"path\": \"pickups\", \"query\": {\"range\": {\"pickups.arrival\": {\"gte\": \"2021-01-01 00:00:00\", \"lt\": \"2021-01-31 23:59:59\", \"time_zone\": \"+08:00\"}}}}}, {\"range\": {\"created\": {\"gte\": \"2020-02-02T05:36:37.761Z\", \"lte\": \"2021-02-02T05:36:37.761Z\", \"format\": \"strict_date_optional_time\"}}}], \"should\": [], \"must_not\": []}}, \"aggs\": {\"Trips\": {\"terms\": {\"field\": \"trip_number\", \"size\": 10000}}}}"
```

Get trip number and use for query
```{r}

data = Search(conn, index = 'tmsuite_booking*', body = bulk_bookings, , raw = TRUE)

data = fromJSON(data)

buckets = data$aggregations$Trips$buckets

key_buckets = buckets$key

```

Combine all query
```{r}
all_trips_data = data.frame()

for(i in 1:length(key_buckets)){
      d = NULL
      trip_query = "{\"size\": 500, \"query\": {\"bool\": {\"must\": [], \"filter\": [{\"bool\": {\"filter\": [{\"bool\": {\"should\": [{\"match_phrase\": {\"trip_number\": \"to_sub\"}}], \"minimum_should_match\": 1}}, {\"bool\": {\"should\": [{\"match_phrase\": {\"data_complete\": \"true\"}}], \"minimum_should_match\": 1}}]}}], \"should\": [], \"must_not\": []}}}"
      
      trip_sub = str_replace(trip_query, 'to_sub', key_buckets[i])
      
      
      trip_data = Search(conn, index = 'tmsuite_booking*', body = trip_sub, , raw = TRUE)
      trip_data = fromJSON(trip_data)$hits$hits$`_source`
      trip_data$i = i
      all_trips_data = dplyr::bind_rows(all_trips_data, trip_data)
}

all_trips_data
```


## Process trip data



Select initial columns
```{r}
processed_data = all_trips_data %>%
      select(trip_number,so_number, pickups, dropoffs, client_name, vehicle_plate_no)
```

Unnest columns
```{r}
processed_data = processed_data %>%
      unnest_wider(dropoffs, names_repair = tidyr_legacy) %>%
      unnest_wider(pickups, names_repair =  tidyr_legacy)
```

Select important columns
```{r}
processed_data = processed_data %>%
      select(trip_number,
             so_number,
             vehicle_plate_no,
             client_name,
             pickup_arrival = arrival,
             pickup_name = name,
             pickup_departure = departure,
             dropoff_arrival = arrival1,
             dropoff_name = name1,
             )
```


Add 8 hours to pickup arrival and dropoff arrival
```{r}
processed_data = processed_data %>%
      mutate(pickup_arrival = ymd_hms(pickup_arrival) + hours(8),
             dropoff_arrival = ymd_hms(dropoff_arrival) + hours(8))
```

```{r}
processed_data_dates = processed_data$pickup_arrival
processed_data_dates = as.character(processed_data_dates)
```

```{r}
processed_data %>%
      write_csv('processed_bookings.csv')
```


```{r}
processed_data = read_csv('processed_bookings.csv')
```

Gather dropoff or pickup
```{r}
processed_data_gathered = processed_data %>%
   select(-pickup_departure) %>%
   gather(key = 'location_type', value = 'location', pickup_name, dropoff_name) %>%
   gather(key = 'time_type', value = 'time', pickup_arrival, dropoff_arrival) %>%
   arrange(trip_number) %>%
   mutate(same = ifelse(
      (str_starts(location_type, 'pickup') & str_starts(time_type, 'pickup')) |
       (str_starts(location_type, 'dropoff') & str_starts(time_type, 'dropoff')),
      1,0
   )) %>%
   filter(same == 1)
```

```{r}
min_max_time = processed_data_gathered %>%
   summarise(min_time = min(time),
             max_time = max(time))

min_max_time$min_time
```

```{r}
min_max_time$min_time_minus_3 = min_max_time$min_time - days(3)
min_max_time$max_time_plus_3 = min_max_time$min_time + days(3)
min_max_time
```

Query date range
```{r}

```


# Get data per timestamp

```{r}
pu_after_query = "{\"size\": 500, \"query\": {\"bool\": {\"must\": [], \"filter\": [{\"bool\": {\"filter\": [{\"bool\": {\"should\": [{\"match_phrase\": {\"group_names\": \"PAN Logistics\"}}], \"minimum_should_match\": 1}}, {\"bool\": {\"should\": [{\"match_phrase\": {\"alert_msgs\": \"entered\"}}], \"minimum_should_match\": 1}}]}}, {\"range\": {\"datestamp\": {\"gte\": \"2021-01-09 20:00:00\", \"time_zone\": \"Asia/Singapore\"}}}], \"should\": [], \"must_not\": []}}, \"sort\": [{\"datestamp\": {\"order\": \"asc\"}}]}"
```

```{r}
data = Search(conn, index = 'tmsuite_triplogs_20*', body = pu_after_query, , raw = TRUE)

data = fromJSON(data)
data = data$hits$hits$`_source`
```




